dist2 <-  dist(rbind(center2 , train[i,]) , method = "euclidean")
dist3 <-  dist(rbind(center3 , train[i,]) , method = "euclidean")
distance_from_centers <- c(dist1 , dist2 , dist3)
train$clusters[i] <- which.min(distance_from_centers)
}
}
#renaming the clusters
train$clusters[train$clusters == 1] <- "setosa"
train$clusters[train$clusters == 2] <- "versicolor"
train$clusters[train$clusters == 3] <- "virginica"
Species <- train_f$Species
#training accuracy
train <- cbind(train , Species)
count <- 0
for (i in 1:nrow(train)){
if(train[i,"Species"] == train[i,"clusters"]){
count <- count+1
}
}
#printing accuracy
print((count/nrow(train))*100)
#now we see the accuracy for test data.
#so first we assign them to clusters using the previous centroids
for (i in 1:nrow(test)){
dist1 <-  dist(rbind(center1 , test[i,]) , method = "euclidean")
dist2 <-  dist(rbind(center2 , test[i,]) , method = "euclidean")
dist3 <-  dist(rbind(center3 , test[i,]) , method = "euclidean")
distance_from_centers <- c(dist1 , dist2 , dist3)
test$clusters[i] <- which.min(distance_from_centers)
}
#renaming the clusters
test$clusters[test$clusters == 1] <- "setosa"
test$clusters[test$clusters == 2] <- "versicolor"
test$clusters[test$clusters == 3] <- "virginica"
Species <- test_f$Species
test <- cbind(test , Species)
#calculating accuracy
count <- 0
for (i in 1:nrow(test)){
if(test[i,"Species"] == test[i,"clusters"]){
count <- count+1
}
}
#printing accuracy
print((count/nrow(test))*100)
#printing the number of times the iteration ran
print(paste(c("The number of iteratoins were " , proof_count)) , sep = " : ")
}
our_fun(train,test,train_f,test_f) #running the proposed K-Means Algorithm
pre_fun(train,test,train_f,test_f) #running the standard K-Means Algorithm
##Kathal Aditya Rajendra 18BCD7008
#Paper doi : https://doi.org/10.1016/j.eswa.2008.01.039
#K-Medoids Paper Implementation
library(dplyr)
library(nlme)
library(factoextra)
library(caret)
set.seed(12) #Setting seeds so that results are re producible.
df <- iris #importing dataset
trainIndex <- createDataPartition(df$Species, p = .7,list = FALSE,times = 1)
train_f <- df[ trainIndex,] #training data split
test_f <- df[-trainIndex,] #testing data split
train <- train_f %>% select(-Species)
test <- test_f %>% select(-Species)
k <- 3 #number of clusters
#step1
#There we are creating the initial distance matrix
#This matrix will be used in future to reduce the time
#required to calcualte distance and also reduce
#time for inclusion of new data points
n = nrow(train)
#The matrix that will hold the data
dist_mat <- matrix(data = NA , nrow = n , ncol = n)
for(i in 1:n){
for(j in i:n){
if(is.na(dist_mat[i,j])){
dist_mat[i,j] <- dist(rbind(train[i,] , train[j,]) , method = "euclidean")
dist_mat[j,i] <- dist_mat[i,j]
}
}
}
#calculation of vj. This value will help us in selecting
#proper initial clusters and thus the algorithm will converge faster
train$vj <- c(0)
train$cluster <- c(-1)
train$seq <- seq(from = 1 , to = n , by = 1)
for(j in 1:n){
vj = 0
for(i in 1:n){
num <- dist_mat[i,j]
denom <- sum(dist_mat[i,])
vj = vj + (num/denom)
}
train[j,"vj"] <- vj
}
#sorting the data based on vj value and select top k clusters as the initial clusters
centroid_select <- train %>% arrange(vj) %>% head(k)
#matrix to store the centroid of each cluster
c_dist <- matrix(data = NA , nrow = 1 , ncol = k)
#Assigning the initial clusters to all the rows using the initial centroids
for(i in 1:n){
csin_dist <- matrix(data = NA , nrow = 1 , ncol = k)
for(j in 1:k){
#here for distance we can directly refer to the dist matrix
csin_dist[1,j] <- dist_mat[i,centroid_select[j,"seq"]]
}
#assigning the cluster to a data point
train[i,"cluster"] <- which.min(csin_dist[1,])
index <- which.min(csin_dist[1,])
#storing that cluster information in the c_dist for future reference
c_dist[1,index] <- ifelse(is.na(c_dist[1,index]) ,min(csin_dist[1,]), c_dist[1,index]+ min(csin_dist[1,]))
}
#While to run until the sum of dist from centroids is almost same
while(TRUE){
#storing prev_sum for future use
prev_sum <- sum(c_dist[1,])
#step2
#Now we check for new centroid in the data set
#Here we look for a new centroids in the present cluster only
#Thus we are decreasing the number of iteration required
for(i in 1:k){
new_cen_dist <- matrix(data = NA , nrow = 1 , ncol = n)
#points to check for.
points_to_consider <- train %>% filter(cluster == i) %>% select(seq)
for(j in 1:n){
for(z in points_to_consider){
#storing distance from each point in the point
#again distance calcualtion is avoided due to distance matrix
new_cen_dist[1,j] <- sum(dist_mat[z,j])
}
}
#finding the point which provided the min sum of distances of all
new_min <- min(new_cen_dist[1,])
pos <- which.min(new_cen_dist[1,])
#checking the the sum of distances is  minimum or not
#If less than prev min then update
if(new_min < c_dist[1,i]){
centroid_select[i,] <- train[pos,]
}
#step3
#Assigning each row to its nearest mediod
c_dist <- matrix(data = 0 , nrow = 1 , ncol = k)
for(i in 1:n){
csin_dist <- matrix(data = NA , nrow = 1 , ncol = k)
for(j in 1:k){
csin_dist[1,j] <- dist_mat[i,centroid_select[j,"seq"]]
}
train[i,"cluster"] <- which.min(csin_dist[1,])
index <- which.min(csin_dist[1,])
#update sum for all rows
c_dist[1,index] <- c_dist[1,index] + min(csin_dist[1,])
}
}
#break condition if no change in sum
if(sum(c_dist) == prev_sum){
break
}
}
#now for test data using the previous mediods
n <- nrow(test)
cluster <- rep.int(0 , n)
centroid_select_test <- centroid_select %>% select(-c("vj","cluster" , "seq"))
c_dist <- matrix(data = 0 , nrow = 1 , ncol = k)
#Assignment of cluster to data points based on previous mediods
for(i in 1:n){
csin_dist <- matrix(data = 0 , nrow = 1 , ncol = k)
for(j in 1:k){
csin_dist[1,j] <- dist(rbind(test[i,] , centroid_select_test[j,]) , method = "euclidean")
}
cluster[i] <- which.min(csin_dist[1,])
}
#plotting of all the data
test$cluster <- as.factor(cluster)
train$cluster <- as.factor(train$cluster)
train$Species <- train_f$Species
test$Species <- test_f$Species
test$seq <- seq(1,n,1)
plot_data_train <-  train %>% group_by(cluster)
plot_data_test <-  test %>% group_by(cluster)
#plot of train data and our prediction class
ggplot(plot_data_train, aes(x= seq , y= Petal.Length , shape= cluster , color=cluster)) + geom_point()
#plot of train data and true class
ggplot(plot_data_train, aes(x= seq , y= Petal.Length , shape= Species , color=Species)) + geom_point()
#plot of test data and our prediction class
ggplot(plot_data_test, aes(x= seq , y= Petal.Length , shape= cluster , color=cluster)) + geom_point()
#plot of test data and true class
ggplot(plot_data_test, aes(x= seq , y= Petal.Length , shape= Species , color=Species)) + geom_point()
#Kathal Aditya Rajendra
library(ggplot2)
library(dplyr)
library(caret)
library(reshape2)
library(boot)
library(moments)
library(broom)
set.seed(7008)
data <- read.csv("mlr.csv")
str(data)
colnames(data) <- c("RD" , "Admin", "MS" , "State" ,"Profit")
#First we see the basic things like if na is present
#number of rows and columns
#mean and median of all numerical columns
#number of states present
nrow(data)
ncol(data)
sum(is.na(data))
summary(data)
#First we try to look that the distribution of data in each numerical column
#We try to see visually if any skewness is present.
ggplot(data, aes(x=RD)) +
geom_histogram(aes(y=..density..), colour="black", fill="white" , bins = 15)+
geom_density(alpha=.2, fill="#FF6666")+
geom_vline(aes(xintercept=mean(RD)),color="blue", linetype="dashed", size=1)
#From the plot we see that the data is normally distributed. So we do not need to
#check the numerical value of the skewness. So now we need to see that the distribution
#is in different states.
ggplot(data, aes(x=RD , color = State)) +
geom_histogram(aes(y=..density..), fill = "white" , bins = 15 , position = "identity")+
geom_density(alpha=.2, fill="#FF6666")
data %>%
group_by(State) %>%
summarise(mean_value = mean(RD)) %>%
arrange(mean_value) %>%
ggplot(aes(x=State, y= mean_value, color= State)) +
geom_bar(stat="identity", fill="white")
#Now we see will see Administration
ggplot(data, aes(x=Admin)) +
geom_histogram(aes(y=..density..), colour="black", fill="white" , bins = 10)+
geom_density(alpha=.2, fill="#FF6666")+
geom_vline(aes(xintercept=mean(Admin)),color="blue", linetype="dashed", size=1)
#Again the data has a normal distribution. So no need to check the skewness value
#numerically Now again we see its distribution in different states
ggplot(data, aes(x=Admin , color = State)) +
geom_histogram(aes(y=..density..), fill = "white" , bins = 10 , position = "identity")+
geom_density(alpha=.2, fill="#FF6666")
data %>%
group_by(State) %>%
summarise(mean_value = mean(Admin)) %>%
arrange(mean_value) %>%
ggplot(aes(x=State, y= mean_value, color= State)) +
geom_bar(stat="identity", fill="white")
#Marketing
ggplot(data, aes(x=MS)) +
geom_histogram(aes(y=..density..), colour="black", fill="white" , bins = 10)+
geom_density(alpha=.2, fill="#FF6666")+
geom_vline(aes(xintercept=mean(MS)),color="blue", linetype="dashed", size=1)
#The visual inspection shows that the predictor is skewed. So we check the numerical
#value of the skewness
skewness(data$MS)
#Now we see state wise distribution
ggplot(data, aes(x= MS , color = State)) +
geom_histogram(aes(y=..density..), fill = "white" , bins = 10 , position = "identity")+
geom_density(alpha=.2, fill="#FF6666")
data %>%
group_by(State) %>%
summarise(mean_value = mean(MS)) %>%
arrange(mean_value) %>%
ggplot(aes(x=State, y= mean_value, color= State)) +
geom_bar(stat="identity", fill="white")
#Now we have seen all the numerical predicators singular. Wee would like to now undestand
#how each one of them affeat the profits.
#RD
ggplot(data, aes(x=RD, y=Profit, shape=State, color=State)) +
geom_point()
#The points show a proper linear regression between RD and profit.Also the data
#across state is uniformly distributed.
ggplot(data, aes(x=Admin, y=Profit, shape=State, color=State)) +
geom_point()
#The points are randomly distributed.So this predicator is not a good
#variable for linear regression related model.Data distribution across
#states is again uniform
ggplot(data, aes(x=Admin, y=Profit, shape=State, color=State)) +
geom_point()
#Now we see for outliners using the box plot method
boxplot(data$RD)
ggplot(data, aes(x= State , y= RD, color=State)) +
geom_boxplot()
boxplot(data$Admin)
ggplot(data, aes(x= State , y= Admin, color=State)) +
geom_boxplot()
#For the total distribution we see no outliners. But for state wise distribution we see
#two points as the outliners. If these points come as outliner in our second test also
# we will remove them.
boxplot(data$MS)
ggplot(data, aes(x= State , y= MS, color=State)) +
geom_boxplot()
#The whole dataset visualized in a single graph
pairs(data %>% select(-State))
#Now we check the outliners using the second method that is WINSORIZATION METHOD
WM <- function(x){
q1 <- quantile(x , c(.01))
q3 <- quantile(x , c(.99))
count <- 1
for (i in x){
if (i < q1 || i > q3){
print(paste(c(i,count)))
}
count <- count+1
}
}
WM(data$RD)
WM(data$Admin)
WM(data$MS)
#From the second method we can see that exactly the same amount of outliners are present
#in the dataset. The row one is outliner in two variable. SO we need to remove it. Since
#Admin is not a good predicator for our model so we are going to check its importance
#if high we will remove those rows else keep them.
data <- data[2:nrow(data), ]
#We have only 3 different states. So we can do one hot encoding.
dummy <- dummyVars(" ~ .", data=data)
newdata <- data.frame(predict(dummy, newdata = data))
# define the control using a random forest selection function
control <- rfeControl(functions=rfFuncs, method="cv", number=100)
# run the RFE algorithm
results <- rfe(newdata[,1:6], newdata[,7], size = c(1:6) , rfeControl=control)
# summarize the results
print(results)
# list the chosen features
predictors(results)
# plot the results
plot(results, type=c("g", "o"))
#Now we look at the correaltion method
cormat <- round(cor(newdata),2)
melted_cormat <- melt(cormat)
ggplot(data = melted_cormat, aes(x=Var1, y=Var2, fill=value)) +
geom_tile() +
geom_text(aes(Var2, Var1, label = value), color = "black", size = 4)
#From using both the analysis above we can clearly see that marketing is highly correlated
#with profit.So we can drop it. Also from the result of RFE we can see that RD is the
#important variable. Thus
newdata <- newdata %>% select(-MS , -StateNew.York , -StateFlorida , -StateCalifornia)
#0-1 normalization
newdata$RD <- (newdata$RD - min(newdata$RD))/(max(newdata$RD)-min(newdata$RD))
newdata$Admin <- (newdata$Admin-min(newdata$Admin))/(max(newdata$Admin)-min(newdata$Admin))
## 70% of the sample size
smp_size <- floor(0.7 * nrow(newdata))
train_ind <- sample(seq_len(nrow(newdata)), size = smp_size)
train <- newdata[train_ind, ]
test <- newdata[-train_ind, ]
model <- lm(formula = Profit~. , data = train)
#ploting the model
plot(model)
MSE = mean((Profit - pred)^2),
RMSE = sqrt(MSE),
MAE = mean(abs(Profit - pred)))
# We have acheived a good accuracy from our model. Thus linear regression was a
#Kathal Aditya Rajendra
library(ggplot2)
library(dplyr)
library(caret)
library(reshape2)
library(boot)
library(moments)
library(broom)
set.seed(7008)
data <- read.csv("mlr.csv")
str(data)
colnames(data) <- c("RD" , "Admin", "MS" , "State" ,"Profit")
#First we see the basic things like if na is present
#number of rows and columns
#mean and median of all numerical columns
#number of states present
nrow(data)
ncol(data)
sum(is.na(data))
summary(data)
#First we try to look that the distribution of data in each numerical column
#We try to see visually if any skewness is present.
ggplot(data, aes(x=RD)) +
geom_histogram(aes(y=..density..), colour="black", fill="white" , bins = 15)+
geom_density(alpha=.2, fill="#FF6666")+
geom_vline(aes(xintercept=mean(RD)),color="blue", linetype="dashed", size=1)
#From the plot we see that the data is normally distributed. So we do not need to
#check the numerical value of the skewness. So now we need to see that the distribution
#is in different states.
ggplot(data, aes(x=RD , color = State)) +
geom_histogram(aes(y=..density..), fill = "white" , bins = 15 , position = "identity")+
geom_density(alpha=.2, fill="#FF6666")
data %>%
group_by(State) %>%
summarise(mean_value = mean(RD)) %>%
arrange(mean_value) %>%
ggplot(aes(x=State, y= mean_value, color= State)) +
geom_bar(stat="identity", fill="white")
#Now we see will see Administration
ggplot(data, aes(x=Admin)) +
geom_histogram(aes(y=..density..), colour="black", fill="white" , bins = 10)+
geom_density(alpha=.2, fill="#FF6666")+
geom_vline(aes(xintercept=mean(Admin)),color="blue", linetype="dashed", size=1)
#Again the data has a normal distribution. So no need to check the skewness value
#numerically Now again we see its distribution in different states
ggplot(data, aes(x=Admin , color = State)) +
geom_histogram(aes(y=..density..), fill = "white" , bins = 10 , position = "identity")+
geom_density(alpha=.2, fill="#FF6666")
data %>%
group_by(State) %>%
summarise(mean_value = mean(Admin)) %>%
arrange(mean_value) %>%
ggplot(aes(x=State, y= mean_value, color= State)) +
geom_bar(stat="identity", fill="white")
#Marketing
ggplot(data, aes(x=MS)) +
geom_histogram(aes(y=..density..), colour="black", fill="white" , bins = 10)+
geom_density(alpha=.2, fill="#FF6666")+
geom_vline(aes(xintercept=mean(MS)),color="blue", linetype="dashed", size=1)
#The visual inspection shows that the predictor is skewed. So we check the numerical
#value of the skewness
skewness(data$MS)
#Now we see state wise distribution
ggplot(data, aes(x= MS , color = State)) +
geom_histogram(aes(y=..density..), fill = "white" , bins = 10 , position = "identity")+
geom_density(alpha=.2, fill="#FF6666")
data %>%
group_by(State) %>%
summarise(mean_value = mean(MS)) %>%
arrange(mean_value) %>%
ggplot(aes(x=State, y= mean_value, color= State)) +
geom_bar(stat="identity", fill="white")
#Now we have seen all the numerical predicators singular. Wee would like to now undestand
#how each one of them affeat the profits.
#RD
ggplot(data, aes(x=RD, y=Profit, shape=State, color=State)) +
geom_point()
#The points show a proper linear regression between RD and profit.Also the data
#across state is uniformly distributed.
ggplot(data, aes(x=Admin, y=Profit, shape=State, color=State)) +
geom_point()
#The points are randomly distributed.So this predicator is not a good
#variable for linear regression related model.Data distribution across
#states is again uniform
ggplot(data, aes(x=Admin, y=Profit, shape=State, color=State)) +
geom_point()
#Now we see for outliners using the box plot method
boxplot(data$RD)
ggplot(data, aes(x= State , y= RD, color=State)) +
geom_boxplot()
boxplot(data$Admin)
ggplot(data, aes(x= State , y= Admin, color=State)) +
geom_boxplot()
#For the total distribution we see no outliners. But for state wise distribution we see
#two points as the outliners. If these points come as outliner in our second test also
# we will remove them.
boxplot(data$MS)
ggplot(data, aes(x= State , y= MS, color=State)) +
geom_boxplot()
#The whole dataset visualized in a single graph
pairs(data %>% select(-State))
#Now we check the outliners using the second method that is WINSORIZATION METHOD
WM <- function(x){
q1 <- quantile(x , c(.01))
q3 <- quantile(x , c(.99))
count <- 1
for (i in x){
if (i < q1 || i > q3){
print(paste(c(i,count)))
}
count <- count+1
}
}
WM(data$RD)
WM(data$Admin)
WM(data$MS)
#From the second method we can see that exactly the same amount of outliners are present
#in the dataset. The row one is outliner in two variable. SO we need to remove it. Since
#Admin is not a good predicator for our model so we are going to check its importance
#if high we will remove those rows else keep them.
data <- data[2:nrow(data), ]
#We have only 3 different states. So we can do one hot encoding.
dummy <- dummyVars(" ~ .", data=data)
newdata <- data.frame(predict(dummy, newdata = data))
# define the control using a random forest selection function
control <- rfeControl(functions=rfFuncs, method="cv", number=100)
?rfe
# run the RFE algorithm
results <- rfe(newdata[,1:6], newdata[,7], size = c(1:6) , rfeControl=control)
# summarize the results
print(results)
# list the chosen features
predictors(results)
# plot the results
plot(results, type=c("g", "o"))
#Now we look at the correaltion method
cormat <- round(cor(newdata),2)
melted_cormat <- melt(cormat)
ggplot(data = melted_cormat, aes(x=Var1, y=Var2, fill=value)) +
geom_tile() +
geom_text(aes(Var2, Var1, label = value), color = "black", size = 4)
#From using both the analysis above we can clearly see that marketing is highly correlated
#with profit.So we can drop it. Also from the result of RFE we can see that RD is the
#important variable. Thus
newdata <- newdata %>% select(-MS , -StateNew.York , -StateFlorida , -StateCalifornia)
#0-1 normalization
newdata$RD <- (newdata$RD - min(newdata$RD))/(max(newdata$RD)-min(newdata$RD))
newdata$Admin <- (newdata$Admin-min(newdata$Admin))/(max(newdata$Admin)-min(newdata$Admin))
## 70% of the sample size
smp_size <- floor(0.7 * nrow(newdata))
train_ind <- sample(seq_len(nrow(newdata)), size = smp_size)
train <- newdata[train_ind, ]
test <- newdata[-train_ind, ]
model <- lm(formula = Profit~. , data = train)
#ploting the model
plot(model)
test$pred <- predict(model , test[,1:2] ,type = "response")
test %>%
summarise(
R2 = cor(Profit, pred)^2,
MSE = mean((Profit - pred)^2),
RMSE = sqrt(MSE),
MAE = mean(abs(Profit - pred)))
# We have acheived a good accuracy from our model. Thus linear regression was a
# We have acheived a good accuracy from our model. Thus linear regression was a
#good fit to this data.
